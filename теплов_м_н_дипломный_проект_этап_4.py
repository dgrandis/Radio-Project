# -*- coding: utf-8 -*-
"""Теплов М.Н. - Дипломный проект - этап 4

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qT_2eYAQwOJ9r0RW_5DhRlWLrYrQYNsV

# **Этап № 4** Создание прототипа НС и получение первой точности распознавания

# **Задание**

Необходимо подговить отчет, который должен содержать:

  1. Тема, описание задачи
  2. База (если нет конфиденциальных данных)
  3. Параметризация данных
  4. Архитектура нейросети
  5. Графическое подтверждение (графики обучения…)
  6. Ноутбук (или .py файл)
  7. Выводы
  8. План дальнейшей работы

# 1. Тема, описание задачи

**Тема дипломного проекта**: Определение признака музыкальных композиций в аудиопотоке(радиоэфире).

**Описание задачи**: Уменьшение громкости аудиопотока при отсутствии признаков музыкальных композиций в радиоэфире.

При появлении признаков речевой информации или рекламного блока происходит уменьшение громкости, как только начинается музыкальная композиция громкость увеличивается до прежнего уровня.

# 2. База (если нет конфиденциальных данных)

Данная база является частичной. Для создания прототипа НС и ускорения обучения.

По **72** файла для музыкальных композиций и рекламы, разговоров.

https://drive.google.com/drive/folders/1-4VnjogDwDMIOdNhlEIsEckSLtcbG9kv?usp=sharing

Полная обучающая база будет на другом аккаунте, так как занимает более 10Гб. И будет предоставлена в следующем этапе при финальной версии НС.

# 3. Параметризация данных

## 3.1 Импорт бибилиотек
"""

from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.layers import Dense, Input, Conv2D, Flatten, Dropout, BatchNormalization
from tensorflow.keras.layers import Conv2DTranspose, Reshape, LeakyReLU, concatenate, MaxPooling2D
from tensorflow.keras.optimizers import Adam, RMSprop
from tensorflow.keras.utils import plot_model, to_categorical
from tensorflow.keras.callbacks import ReduceLROnPlateau

from sklearn.preprocessing import StandardScaler 
from sklearn.model_selection import train_test_split
from sklearn.decomposition import MiniBatchDictionaryLearning

import librosa
import IPython.display as ipd
import scipy.stats
import numpy as np
import os
import random
from time import time
import matplotlib.pyplot as plt
from tqdm import tqdm

from google.colab import drive
drive.mount('/content/drive')

"""## 3.2 Примеры записей

Послушаем примеры записей

### 3.2.1 Музыкальная композиция
"""

ipd.Audio('/content/drive/MyDrive/Диплом/Music/1.wav')

"""### 3.2.2 Реклама"""

ipd.Audio('/content/drive/MyDrive/Диплом/Speech_radio/2.wav')

"""## 3.3 Подготовка файлов

Взглянем на содержимое папок

### 3.3.1 Музыкальные произведения (**72 файла**)
"""

!ls /content/drive/MyDrive/УИИ/Диплом/Music

"""### 3.3.2 Реклама и речь (**72 файла**)"""

!ls /content/drive/MyDrive/УИИ/Диплом/Speech_radio

"""## 3.4 Базовые константы"""

DATA_PATH = '/content/drive/MyDrive/Диплом/'

#  Задаем стандартные параметры для всех аудиофайлов

classes= ['Music', 'Speech_radio']   # Классы
n_classes = len(classes)             # Число классов
sample_rate = 22050
step_mfcc = int(0.05 * sample_rate)  # 0.05
channel = 1                          # Количество каналов

"""## 3.5 Создание необходимый функций"""

# Функция извлечения лейблов классов
def get_labels(path = DATA_PATH):
  labels = sorted(os.listdir(path))                            # Список папок
  label_indices = np.arange(0, len(labels))                    # Массив длинной, по количеству классов
  return labels, label_indices, to_categorical(label_indices)  # Возврат значений

# Функция параметризации аудио
# Данная функция получает на вход аудиофайл. Нарезает его на отрезки
# С помощью бибилиотеки librosa выделяет фичи и возвращает numpy массив

def wav2mfcc(file_path, length=22050, step=22050):  #length=11025
  out_mfcc = []
  
  y, sr = librosa.load(file_path)
  
  while (len(y) > length):
    out_section = []

    if len(y) > length:
      section = y[:length]
      section = np.array(section)
    else:
      section = y[:len(y)]
      section = np.array(section)

    mfcc = librosa.feature.mfcc(section, sr)
    chroma_stft = librosa.feature.chroma_stft(section, sr=sr)

    rms = librosa.feature.rms(section)
    spec_cent = librosa.feature.spectral_centroid(section, sr=sr)
    spec_bnw = librosa.feature.spectral_bandwidth(section, sr=sr)
    rolloff = librosa.feature.spectral_rolloff(section, sr=sr)
    zcr = librosa.feature.zero_crossing_rate(section)

    onset_subbands = librosa.onset.onset_strength_multi(section, sr=sr) #, channels=[0, 32, 64, 96, 128]) # Вычисление огибающую силы начала спектрального потока по нескольким каналам

    # Спектральные особенности
    chroma_cqt = librosa.feature.chroma_cqt(section, sr=sr)         # Constant-Q хроматограмма
    chroma_cens = librosa.feature.chroma_cens(section, sr=sr)       # Вычисление варианта цветности «Нормализованная энергия цветности» (CENS)
    melspectrogram = librosa.feature.melspectrogram(section, sr=sr) # Вычисление спектрограммы в масштабе мела.
    spectral_contrast = librosa.feature.spectral_contrast(section)  # Вычисление спектрального контраста
    poly_features = librosa.feature.poly_features(section, sr=sr)   # Получение коэффициента подгонки полинома n-го порядка к столбцам спектрограммы.
    tonnetz = librosa.feature.tonnetz(section, sr=sr)

    out_section.extend(mfcc)
    out_section.extend(chroma_stft)
    out_section.extend(rms)
    out_section.extend(spec_cent)
    out_section.extend(spec_bnw)
    out_section.extend(rolloff)
    out_section.extend(zcr)
    out_section.extend(onset_subbands)
    out_section.extend(chroma_cqt)
    out_section.extend(chroma_cens)
    out_section.extend(melspectrogram)
    out_section.extend(spectral_contrast)
    out_section.extend(poly_features)
    out_section.extend(tonnetz)
    out_mfcc.append(out_section)
    y = y[step:]
  
  out_mfcc = np.array(out_mfcc)
  return out_mfcc

# Функция формирования и сохранения векторов данных
def save_data_to_array(path=DATA_PATH, length=22050, step_mfcc = 22050): # length=11025
  labels, _, _ = get_labels(path)
  for label in labels:
    mfcc_vectors = []
    wavfiles = [path + label + '/' + wavfile for wavfile in os.listdir(path + label)]
    for wavfile in tqdm(wavfiles,  f"Сохраняем векторы класса - {label}"):
      print(wavfile)
      mfcc = wav2mfcc(wavfile, length=length, step = step_mfcc)
      if mfcc.shape[0] != 0:
        mfcc_vectors.extend(mfcc)
    np.save(label + '.npy', mfcc_vectors)

"""## 3.6 Создание numpy массивов с признаками"""

# Вызываем функцию для получения numpy массивов необходимых для обучения нейроннной сети
save_data_to_array()

"""Сохранение на гугл диск"""

from shutil import copyfile

copyfile('/content/Music.npy', '/content/drive/MyDrive/Диплом/Music.npy')

copyfile('/content/Speech_radio.npy', '/content/drive/MyDrive/Диплом/Speech_radio.npy')

# copyfile('/content/drive/MyDrive/Диплом/Speech_radio.npy', '/content/Speech_radio.npy')
# copyfile('/content/drive/MyDrive/Диплом/Music.npy', '/content/Music.npy')

"""## 3.7 Загрузка numpy массивов (eсли требуется)"""

Music  = np.load('/content/Music.npy')
Speech = np.load('/content/Speech_radio.npy')

print(Music.shape)
print(Speech.shape)

"""## 3.8 Формирование обучающейи тестовой выборкок"""

# Функция для получения обучающей/проверочной выборки
def get_train_test(split_ratio=0.8, random_state=42, colab=True):
  labels, indices, _ = get_labels(DATA_PATH)

  if colab:
    X = np.load('/content/Music.npy')
  else:
    X = np.load('/content/drive/MyDrive/Диплом/Music.npy')
  y = np.zeros(X.shape[0], dtype='int32')

  for i, label in enumerate(labels[1:]):
    if colab:
      x = np.load(label + '.npy')
    else:
      x = np.load('/content/drive/MyDrive/Диплом/' + label + '.npy')

    X = np.vstack((X, x))
    y = np.append(y, np.full(x.shape[0], fill_value= (i + 1)))
  
  return train_test_split(X, y, test_size=(1-split_ratio), random_state=random_state, shuffle=True)

"""### 3.8.1 Разделение на обучающую и проверочную выборки"""

# Формируем обучающую и проверочную выборки
x_train, x_test, y_train, y_test = get_train_test(colab=True)

print(x_train.shape)
print(y_train.shape)
print()
print(x_test.shape)
print(y_test.shape)

"""### 3.8.2 Добавление дополнительной размерности"""

# Добавляем значение канала
x_train = x_train[..., None]
x_test = x_test[..., None]

print(x_train.shape)
print(x_test.shape)

"""### 3.8.3 Перевод в формат One hot encoding(OHE)"""

y_train_hot = to_categorical(y_train)
y_test_hot = to_categorical(y_test)

print(y_train_hot.shape)
print(y_test_hot.shape)

"""## 3.9 Проверка количества данных"""

# Нулевого класса - Музыкальные композиции
ind = np.where(y_train==0)
ind[0].shape

# Первого класса - Реклама и речь
ind = np.where(y_train==1)
ind[0].shape

"""# 4. Архитектура нейросети

## 4.1 Создание и обучение нейронной сети
"""

# Функция создания модели нейросети
def get_model():
  model = Sequential()
  model.add(Conv2D(8, kernel_size=(3,3), activation='relu', input_shape=(x_train.shape[1], x_train.shape[2], channel)))
  model.add(MaxPooling2D(pool_size=(2,2)))
  model.add(BatchNormalization())
  model.add(Flatten())
  model.add(Dense(128,activation='relu'))
  model.add(Dropout(0.25))
  model.add(BatchNormalization())
  model.add(Dense(n_classes, activation='softmax'))
  model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])
  return model

model = get_model()

"""### 4.1.1 Просмотер summary и архитектуры сети"""

model.summary()

plot_model(model, show_shapes=True, dpi=70)

"""### 4.1.2 Обучение сети"""

history = model.fit(x_train, y_train_hot, batch_size=32, epochs=10, validation_data=(x_test, y_test_hot), verbose=1)

# Сохранение весовых коэффициентов
model.save_weights('/content/model.h4')

# Загрузка весовых коэффициентов
# model.load_weights('/content/model.h4')

"""### 4.1.3 Графики точности и ошибок """

plt.plot(history.history["accuracy"], label='Доля верных ответов на обучающем наборе')
plt.plot(history.history["loss"], label='Доля ошибки на обучающем наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Доля верных ответов')
plt.legend()
plt.show()

plt.plot(history.history["val_accuracy"], label='Доля верных ответов на проверочном наборе')
plt.plot(history.history["val_loss"], label='Доля ошибки на проверочном наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Доля верных ответов')
plt.legend()
plt.show()

"""## 4.2. Изменение **количества** **фильтров** в сверточных слоях"""

model_2 = Sequential()

model_2.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(x_train.shape[1], x_train.shape[2], channel)))
model_2.add(Conv2D(32, kernel_size=(3,3), activation='relu'))
model_2.add(MaxPooling2D(pool_size=(2,2)))
model_2.add(BatchNormalization())

model_2.add(Conv2D(32, kernel_size=(3,3), activation='relu'))
model_2.add(Conv2D(32, kernel_size=(3,3), activation='relu'))

model_2.add(Flatten())
model_2.add(Dense(128,activation='relu'))
model_2.add(Dropout(0.25))
model_2.add(BatchNormalization())
model_2.add(Dense(n_classes, activation='sigmoid'))

model_2.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])

"""### 4.2.1 Просмотер summary и архитектуры сети"""

model_2.summary()

plot_model(model_2, show_shapes=True, dpi=70)

"""### 4.2.2 Обучение сети"""

history_2 = model_2.fit(x_train, y_train_hot, batch_size=32, epochs=10, validation_data=(x_test, y_test_hot), verbose=1)

# Сохранение весовых коэффициентов
model_2.save_weights('/content/model_2.h4')

"""### 4.2.3 Графики точности и ошибок """

plt.plot(history_2.history["accuracy"], label='Доля верных ответов на обучающем наборе')
plt.plot(history_2.history["loss"], label='Доля ошибки на обучающем наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Доля верных ответов')
plt.legend()
plt.show()

plt.plot(history_2.history["val_accuracy"], label='Доля верных ответов на проверочном наборе')
plt.plot(history_2.history["val_loss"], label='Доля ошибки на проверочном наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Доля верных ответов')
plt.legend()
plt.show()

"""## 4.3 Добавление **MaxPooling2D** после второго блока сверточных слоев"""

model_3 = Sequential()

model_3.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(x_train.shape[1], x_train.shape[2], channel)))
model_3.add(Conv2D(32, kernel_size=(3,3), activation='relu'))
model_3.add(MaxPooling2D(pool_size=(2,2)))
model_3.add(BatchNormalization())

model_3.add(Conv2D(32, kernel_size=(3,3), activation='relu'))
model_3.add(Conv2D(32, kernel_size=(3,3), activation='relu'))
model_3.add(BatchNormalization())
model_3.add(MaxPooling2D(pool_size=(2,2)))

model_3.add(Flatten())
model_3.add(Dense(128,activation='relu'))
model_3.add(Dropout(0.25))
model_3.add(BatchNormalization())
model_3.add(Dense(n_classes, activation='sigmoid'))
model_3.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])

"""### 4.3.1 Просмотер summary и архитектуры сети"""

model_3.summary()

plot_model(model_3, show_shapes=True, dpi=70)

"""### 4.3.2 Обучение сети"""

history_3 = model_3.fit(x_train, y_train_hot, batch_size=32, epochs=10, validation_data=(x_test, y_test_hot), verbose=1)

# Сохранение весовых коэффициентов
model_3.save_weights('/content/model_2.h4')

"""### 4.2.3 Графики точности и ошибок """

plt.plot(history_3.history["accuracy"], label='Доля верных ответов на обучающем наборе')
plt.plot(history_3.history["loss"], label='Доля ошибки на обучающем наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Доля верных ответов')
plt.legend()
plt.show()

plt.plot(history_3.history["val_accuracy"], label='Доля верных ответов на проверочном наборе')
plt.plot(history_3.history["val_loss"], label='Доля ошибки на проверочном наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Доля верных ответов')
plt.legend()
plt.show()

"""## 4.4 Функциональное программирование"""

# Попробуем применить другую архитектуру сети на тех же данных.

# Создадим сеть с 7 верками, с разным количеством нейронов и функциями активации.
input1 = Input((x_train.shape[1], x_train.shape[2], channel))

x1 = Dense(64, activation="elu")(input1)
x1 = BatchNormalization()(x1)
x1 = Dropout(0.3)(x1)
x1 = Dense(64, activation="elu")(x1)
x1 = BatchNormalization()(x1)
x1 = Dropout(0.3)(x1)

x2 = Dense(64, activation="elu")(input1)
x2 = BatchNormalization()(x2)
x2 = Dropout(0.3)(x2)
x2 = Dense(40, activation="softmax")(x2)
x2 = BatchNormalization()(x2)
x2 = Dropout(0.3)(x2)

x3 = Dense(64, activation="elu")(input1)
x3 = BatchNormalization()(x3)
x3 = Dropout(0.1)(x3)
x3 = Dense(200, activation="softmax")(x3)
x3 = BatchNormalization()(x3)
x3 = Dropout(0.3)(x3)

x4 = Dense(40, activation="softmax")(input1)
x4 = BatchNormalization()(x4)
x4 = Dropout(0.1)(x4)


x5 = Dense(200, activation="softmax")(input1)
x5 = BatchNormalization()(x5)
x5 = Dropout(0.3)(x5)

x6 = Dense(10, activation="softmax")(input1)
x6 = BatchNormalization()(x6)
x6 = Dropout(0.1)(x6)

x7 = Dense(64, activation="elu")(input1)
x7 = BatchNormalization()(x7)
x7 = Dropout(0.3)(x7)
x7 = Dense(10, activation="softmax")(x7)
x7 = BatchNormalization()(x7)
x7 = Dropout(0.3)(x7)

x = concatenate([x1, x2, x3, x4, x5, x6, x7])

x = Dense(64, activation='elu')(x)
x = Dense(32, activation='elu')(x)

x = MaxPooling2D(pool_size=(4,4))(x)

x = Dense(16, activation='elu')(x)
x = Dense(8, activation='elu')(x)

x = MaxPooling2D(pool_size=(2,2))(x)

x = Flatten()(x)

x = Dense(64, activation='elu')(x)
x = BatchNormalization()(x)
x = Dropout(0.3)(x)
x = Dense(n_classes, activation='sigmoid')(x)

model_4 = Model(input1, x)

# Компилируем
model_4.compile(optimizer=Adam(learning_rate=1e-4),
              loss='binary_crossentropy',
              metrics=['accuracy'])

"""### 4.4.1 Просмотер summary и архитектуры сети"""

model_4.summary()

plot_model(model_4, show_shapes=True, dpi=70)

"""### 4.4.2 Обучение сети"""

history_4 = model_4.fit(x_train, y_train_hot, batch_size=32, epochs=10, validation_data=(x_test, y_test_hot), verbose=1)

# Сохранение весовых коэффициентов
model_4.save_weights('/content/model_4.h4')

"""### 4.4.3 Графики точности и ошибок """

plt.plot(history_4.history["accuracy"], label='Доля верных ответов на обучающем наборе')
plt.plot(history_4.history["loss"], label='Доля ошибки на обучающем наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Доля верных ответов')
plt.legend()
plt.show()

plt.plot(history_4.history["val_accuracy"], label='Доля верных ответов на проверочном наборе')
plt.plot(history_4.history["val_loss"], label='Доля ошибки на проверочном наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Доля верных ответов')
plt.legend()
plt.show()

"""# 5. Графическое подтверждение (графики обучения…)

## 1
"""

plt.plot(history.history["accuracy"], label='Доля верных ответов на обучающем наборе')
plt.plot(history.history["loss"], label='Доля ошибки на обучающем наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Доля верных ответов')
plt.legend()
plt.show()

plt.plot(history.history["val_accuracy"], label='Доля верных ответов на проверочном наборе')
plt.plot(history.history["val_loss"], label='Доля ошибки на проверочном наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Доля верных ответов')
plt.legend()
plt.show()

"""## 2"""

plt.plot(history_2.history["accuracy"], label='Доля верных ответов на обучающем наборе')
plt.plot(history_2.history["loss"], label='Доля ошибки на обучающем наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Доля верных ответов')
plt.legend()
plt.show()

plt.plot(history_2.history["val_accuracy"], label='Доля верных ответов на проверочном наборе')
plt.plot(history_2.history["val_loss"], label='Доля ошибки на проверочном наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Доля верных ответов')
plt.legend()
plt.show()

"""## 3"""

plt.plot(history_3.history["accuracy"], label='Доля верных ответов на обучающем наборе')
plt.plot(history_3.history["loss"], label='Доля ошибки на обучающем наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Доля верных ответов')
plt.legend()
plt.show()

plt.plot(history_3.history["val_accuracy"], label='Доля верных ответов на проверочном наборе')
plt.plot(history_3.history["val_loss"], label='Доля ошибки на проверочном наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Доля верных ответов')
plt.legend()
plt.show()

"""## 4"""

plt.plot(history_4.history["accuracy"], label='Доля верных ответов на обучающем наборе')
plt.plot(history_4.history["loss"], label='Доля ошибки на обучающем наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Доля верных ответов')
plt.legend()
plt.show()

plt.plot(history_4.history["val_accuracy"], label='Доля верных ответов на проверочном наборе')
plt.plot(history_4.history["val_loss"], label='Доля ошибки на проверочном наборе')
plt.xlabel('Эпоха обучения')
plt.ylabel('Доля верных ответов')
plt.legend()
plt.show()

"""# 6. Ноутбук (или .py файл)

Данный ноутбук доступен по ссылке : 
https://colab.research.google.com/drive/1qT_2eYAQwOJ9r0RW_5DhRlWLrYrQYNsV?usp=sharing

А так же на гугл диске: https://drive.google.com/file/d/18zYWIRDSKQ1iyX_zCYqbkCpRoeveDnAU/view?usp=sharing

# 7. Выводы

1. На данном этапе создавался прототип нейронной сети с получением первой точности распознавания.
2. Для данной работы использовалась частично обучающая база, для уменьшения обучения различных архитектур.
3. Как видно из полученных данных нет смысла использовать сложные нейронные сети с большим количеством внутренних слов, так как более простые дают такой же результат.
4. Подготовка и обработка данных занимает основное время работы.

# 8. План дальнейшей работы

1. Параметризация всех данных для обучения
2. Разделение на выборки:
  - Обучающая
  - Проверочная
  - Тестовая
3. Сохранение всех массивов данных на гугл диск
4. Обучение нейронной сети с получением более высокой точности на тестовой выборке, путем:
  - Использования callback для уменьшения шага обучения и сохранения весом
  - Обучение производить на полной выборке
  - Так же можно дообучать на новых данных
5. Создание презентации
6. Отправка на проверку
7. Защита дипломного проекта
8. Интеграция в Production. Создание программы для работы нейронной сети на PC.
"""